{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwK5-9FIB-lu"
   },
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1kiO9kACE6s"
   },
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "O_54McxZPsmT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTfaCIzdCLPA"
   },
   "source": [
    "## Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xkinmOI3PzPj",
    "outputId": "5cc27835-1d23-4221-e25a-1f069949b307"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Liked\n",
      "0                           Wow... Loved this place.      1\n",
      "1                                 Crust is not good.      0\n",
      "2          Not tasty and the texture was just nasty.      0\n",
      "3  Stopped by during the late May bank holiday of...      1\n",
      "4  The selection on the menu was great and so wer...      1\n"
     ]
    }
   ],
   "source": [
    "# quoting = 3, ignores \" to avoid processing errors \n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qekztq71CixT"
   },
   "source": [
    "## Cleaning the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FgapPdjKmrus",
    "outputId": "3461b55a-a25d-4c4a-ee44-eee2cfc45efe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "corpus = []\n",
    "for i in range(0, 1000):\n",
    "  review = re.sub('[^a-zA-Z]', ' ', dataset.values[i, 0])\n",
    "  review = review.lower()\n",
    "  review = review.split() # tokenize (covernt from string to list)\n",
    "\n",
    "  #stemming\n",
    "  ps = PorterStemmer()\n",
    "  all_stopwords = stopwords.words('english')\n",
    "  # not indicates sentiment and therefore should not be part of stop words\n",
    "  all_stopwords.remove('not')\n",
    "  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "  review = ' '.join(review)\n",
    "  corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLqmAkANCp1-"
   },
   "source": [
    "## Creating the Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "id": "Rb9hqbNo5Tz6"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "# tf_idf_vectorizer = TfidfVectorizer(max_features = 1500)\n",
    "# x = tf_idf_vectorizer.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DH_VjgPzC2cd"
   },
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "id": "BCDa9loX5YDm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkIq23vEDIPt"
   },
   "source": [
    "## Training models on the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "FpkIa09t6UCt"
   },
   "outputs": [],
   "source": [
    "# Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression().fit(x_train, y_train)\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2).fit(x_train, y_train)\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='rbf').fit(x_train, y_train)\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB().fit(x_train, y_train)\n",
    "# Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(criterion='entropy').fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoMltea5Dir1"
   },
   "source": [
    "## Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOxW0yiS9G3z",
    "outputId": "43f83996-c3ff-4ab5-d06a-0191ee63af68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression performance\n",
      "[[80 17]\n",
      " [28 75]]\n",
      "0.775\n",
      "KNN performance\n",
      "[[74 23]\n",
      " [45 58]]\n",
      "0.66\n",
      "SVM performance\n",
      "[[89  8]\n",
      " [36 67]]\n",
      "0.78\n",
      "Naive Bayes performance\n",
      "[[55 42]\n",
      " [12 91]]\n",
      "0.73\n",
      "Random Forest performance\n",
      "[[90  7]\n",
      " [41 62]]\n",
      "0.76\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# logistic regression\n",
    "print(\"Logistic Regression performance\")\n",
    "y_pred_lr = lr.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "print(accuracy_score(y_test, y_pred_lr))\n",
    "\n",
    "# knn\n",
    "print(\"KNN performance\")\n",
    "y_pred_knn = knn.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred_knn))\n",
    "print(accuracy_score(y_test, y_pred_knn))\n",
    "\n",
    "# SVM\n",
    "print(\"SVM performance\")\n",
    "y_pred_svc = svc.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred_svc))\n",
    "print(accuracy_score(y_test, y_pred_svc))\n",
    "\n",
    "# Naive Bayes\n",
    "print(\"Naive Bayes performance\")\n",
    "y_pred_nb = nb.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred_nb))\n",
    "print(accuracy_score(y_test, y_pred_nb))\n",
    "\n",
    "# Random Forest\n",
    "print(\"Random Forest performance\")\n",
    "y_pred_rf = rf.predict(x_test)\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(accuracy_score(y_test, y_pred_rf))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of natural_language_processing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
